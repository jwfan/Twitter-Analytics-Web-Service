# mySQL for Query 2
# Step 1 create database
drop database if exists q2_db;
create database q2_db default charset utf8 COLLATE utf8_general_ci;
use q2_db;

# Step 2 create twitter table
drop table if exists `q2_table`;
create table `q2_table` (
	`unique_id` int not null auto_increment,
	`hashtag` varbinary(140) not null,
	`user_id` varchar(19) not null,
	`keywords` LONGTEXT not null,
	primary key (unique_id)
);

# Step 3 add data to businesses table
load data local infile 'part-00000' into table q2_table columns terminated by '\t' LINES TERMINATED BY '\n' (hashtag, user_id, keywords);

# Step 4 create index
create index hashtag_user_index on q2_table (hashtag, user_id);

# HBase for Query 2
create 'twitter_db', 'data', {SPLITS => [‘a’]}

hadoop fs -mkdir /user/input
hadoop fs -put output /user/input
hadoop fs -ls /user/input
hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=HBASE_ROW_KEY,data:userid -Dimporttsv.bulk.output=/user/output twitter /user/input/output
hadoop fs -chown -R hbase:hbase /user/output
hbase org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles /user/output twitter

# mySQL for Query 3
# Step 1 create database
drop database if exists q3_db;
create database q3_db default charset utf8 COLLATE utf8_general_ci;
use q3_db;

# Step 2 create twitter table
drop table if exists `q3_table`;
create table `q3_table` (
	`ut_id` varchar(32) not null,
	`text` LONGTEXT not null,
	primary key (ut_id)
);

# Step 3 add data to businesses table
load data local infile 'part-00000' into table q3_table columns terminated by '\t' LINES TERMINATED BY '\n';

# Step 4 create index
create index hashtag_user_index on q3_table (ut_id);
